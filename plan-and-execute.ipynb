{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b5811c-1074-495f-9722-8325b5e717d3",
   "metadata": {},
   "source": [
    "# Plan-and-Execute\n",
    "\n",
    "This notebook shows how to create a \"plan-and-execute\" style agent. This is heavily inspired by the [Plan-and-Solve](https://arxiv.org/abs/2305.04091) paper as well as the [Baby-AGI](https://github.com/yoheinakajima/babyagi) project.\n",
    "\n",
    "The core idea is to first come up with a multi-step plan, and then go through that plan one item at a time.\n",
    "After accomplishing a particular task, you can then revisit the plan and modify as appropriate.\n",
    "\n",
    "\n",
    "The general computational graph looks like the following:\n",
    "\n",
    "\n",
    "![plan-and-execute diagram](./img/plan-and-execute.png)\n",
    "\n",
    "\n",
    "This compares to a typical [ReAct](https://arxiv.org/abs/2210.03629) style agent where you think one step at a time.\n",
    "The advantages of this \"plan-and-execute\" style agent are:\n",
    "\n",
    "1. Explicit long term planning (which even really strong LLMs can struggle with)\n",
    "2. Ability to use smaller/weaker models for the execution step, only using larger/better models for the planning step\n",
    "\n",
    "\n",
    "The following walkthrough demonstrates how to do so in LangGraph. The resulting agent will leave a trace like the following example: ([link](https://smith.langchain.com/public/d46e24d3-dda6-44d5-9550-b618fca4e0d4/r))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44a72d6-7e0c-4478-9d20-4c09000420a8",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we need to install the packages required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b451b58a-89bd-424f-8c06-0d9fe325e01b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture --no-stderr\n",
    "# %pip install --quiet -U langchain-community langchain-openai tavily-python\n",
    "\n",
    "# %pip install --quiet -U langchain-community \n",
    "# %pip install crewai\n",
    "# %pip install crewai_tools\n",
    "# %pip install -U sqlalchemy\n",
    "#%pip install -q -U duckduckgo-search\n",
    "# %pip install langchain-community langchain-core langgraph\n",
    "\n",
    "# %pip install -q langchainhub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f267b0-98db-4a59-8b2c-a23f795576ff",
   "metadata": {},
   "source": [
    "Next, we need to set API keys for OpenAI (the LLM we will use) and Tavily (the search tool we will use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce438281-08d5-4804-afe7-e4089f7b016b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "# _set_env(\"OPENAI_API_KEY\")\n",
    "# _set_env(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2d7981-3737-4134-8bef-d00d18d4e91d",
   "metadata": {},
   "source": [
    "Optionally, we can set API key for LangSmith tracing, which will give us best-in-class observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c42dd60a-b969-4995-9591-15c66af4ce67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "SVC_ACC = !(gcloud config get-value core/account)\n",
    "SVC_ACC = SVC_ACC[0]\n",
    "\n",
    "PROJECT_NUMBER=str(re.search(r'\\d+', SVC_ACC).group())\n",
    "\n",
    "LOCATION=\"us-central1\"\n",
    "\n",
    "FOLDER_NAME=\".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01f460d1-f26f-47d1-ae76-de74d5d851de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# _set_env(\"LANGCHAIN_API_KEY\")\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = \"Plan-and-execute\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52d852b6-cf72-4c45-94d7-97b605af413f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['TAVILY_API_KEY'] = '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'\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\") # Ensure this is set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5fb09a-0311-44c2-b243-d0e80de78902",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Tools\n",
    "\n",
    "We will first define the tools we want to use. For this simple example, we will use a built-in search tool via Tavily. However, it is really easy to create your own tools - see documentation [here](https://python.langchain.com/docs/modules/agents/tools/custom_tools) on how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25b9ec62-0675-4715-811c-9b32c635b22f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "# Instantiate tools\n",
    "\n",
    "# from crewai import Agent, Task, Crew, Process\n",
    "from crewai_tools import tool\n",
    "from crewai_tools.tools import FileReadTool\n",
    "import os, requests, re, subprocess\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "file_read_tool = FileReadTool(\n",
    "\tfile_path='template.md',\n",
    "\tdescription='A tool to read the Story Template file and understand the expected output format.'\n",
    ")\n",
    "\n",
    "# search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "# tools = [search_tool]\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)\n",
    "\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "tools = [web_search_tool]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcda478-fa80-4e3e-bb35-0f622fe73a31",
   "metadata": {},
   "source": [
    "## Define our Execution Agent\n",
    "\n",
    "Now we will create the execution agent we want to use to execute tasks. \n",
    "Note that for this example, we will be using the same execution agent for each task, but this doesn't HAVE to be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72d233ca-1dbf-4b43-b680-b3bf39e3691f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful assistant.\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{{messages}}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from vertexai.preview.vision_models import ImageGenerationModel\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "import uuid, os\n",
    "\n",
    "# Initialize Gemini LLM\n",
    "llm = ChatVertexAI(\n",
    "    model_name=\"gemini-1.0-pro-002\", # Replace with your desired Gemini model\n",
    "    project_id=os.getenv(PROJECT_ID), # Your Vertex AI project ID\n",
    "    location=\"us-central1\", # Your Vertex AI location\n",
    ")\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"wfh/react-agent-executor\")\n",
    "prompt.pretty_print()\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "# llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "agent_executor = create_react_agent(llm, tools, messages_modifier=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "746e697a-dec4-4342-a814-9b3456828169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='who is the winnner of the female tennis tournament Rome 2024', id='b1d07e31-afa1-4bf0-afe0-0e71d6b5bdc5'),\n",
       "  AIMessage(content='I am sorry, I cannot fulfill this request. The available tools lack the information to answer your question.', response_metadata={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}], 'citation_metadata': None, 'usage_metadata': {'prompt_token_count': 77, 'candidates_token_count': 21, 'total_token_count': 98}}, id='run-68cd87a4-4838-42f6-9c31-82d58f5c8e08-0')]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"messages\": [(\"user\", \"who is the winnner of the female tennis tournament Rome 2024\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf66804-44b2-4904-b1a7-17ad70b551f5",
   "metadata": {},
   "source": [
    "## Define the State\n",
    "\n",
    "Let's now start by defining the state the track for this agent.\n",
    "\n",
    "First, we will need to track the current plan. Let's represent that as a list of strings.\n",
    "\n",
    "Next, we should track previously executed steps. Let's represent that as a list of tuples (these tuples will contain the step and then the result)\n",
    "\n",
    "Finally, we need to have some state to represent the final response as well as the original input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eeeaeea-8f10-4fbe-8e24-4e1a2381a009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Annotated, TypedDict\n",
    "import operator\n",
    "\n",
    "\n",
    "class PlanExecute(TypedDict):\n",
    "    input: str\n",
    "    plan: List[str]\n",
    "    past_steps: Annotated[List[Tuple], operator.add]\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbd770a-9941-40a9-977e-4d55359eee21",
   "metadata": {},
   "source": [
    "## Planning Step\n",
    "\n",
    "Let's now think about creating the planning step. This will use function calling to create a plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a88626d-6dfd-4488-87f0-a9a0dd6da44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[str] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec7b1867-1ea3-4df3-9a98-992a1c32ec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"For the given objective, come up with a simple step by step plan. \\\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "planner = planner_prompt | ChatVertexAI(\n",
    "    model_name=\"gemini-1.0-pro-002\", # Replace with your desired Gemini model\n",
    "    project_id=os.getenv(PROJECT_ID), # Your Vertex AI project ID\n",
    "    location=\"us-central1\", # Your Vertex AI location\n",
    ").with_structured_output(Plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67ce37b7-e089-479b-bcb8-c3f5d9874613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 10:36:24,134 - 140263268149056 - before_sleep.py-before_sleep:65 - WARNING: Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised InternalServerError: 500 Internal error occurred..\n"
     ]
    }
   ],
   "source": [
    "planner.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\"user\", \"what is the hometown of the current Australia open winner?\")\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e09ad9d-6f90-4bdc-bb43-b1ce94517c29",
   "metadata": {},
   "source": [
    "## Re-Plan Step\n",
    "\n",
    "Now, let's create a step that re-does the plan based on the result of the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec2d12cc-016a-44d1-aa08-4c5ce1e8fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Act(BaseModel):\n",
    "    \"\"\"Action to perform.\"\"\"\n",
    "\n",
    "    action: Union[Response, Plan] = Field(\n",
    "        description=\"Action to perform. If you want to respond to user, use Response. \"\n",
    "        \"If you need to further use tools to get the answer, use Plan.\"\n",
    "    )\n",
    "\n",
    "\n",
    "replanner_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"For the given objective, come up with a simple step by step plan. \\\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "Your objective was this:\n",
    "{input}\n",
    "\n",
    "Your original plan was this:\n",
    "{plan}\n",
    "\n",
    "You have currently done the follow steps:\n",
    "{past_steps}\n",
    "\n",
    "Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "replanner = replanner_prompt | ChatVertexAI(\n",
    "    model_name=\"gemini-1.0-pro-002\", # Replace with your desired Gemini model\n",
    "    project_id=os.getenv(PROJECT_ID), # Your Vertex AI project ID\n",
    "    location=\"us-central1\", # Your Vertex AI location\n",
    ").with_structured_output(Act)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859abd13-6ba0-45ad-b341-e652dd5f755b",
   "metadata": {},
   "source": [
    "## Create the Graph\n",
    "\n",
    "We can now create the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c8e0dad-bcea-4c9a-8922-0d820892e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "async def execute_step(state: PlanExecute):\n",
    "    plan = state[\"plan\"]\n",
    "    plan_str = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(plan))\n",
    "    task = plan[0]\n",
    "    task_formatted = f\"\"\"For the following plan:\n",
    "{plan_str}\\n\\nYou are tasked with executing step {1}, {task}.\"\"\"\n",
    "    agent_response = await agent_executor.ainvoke(\n",
    "        {\"messages\": [(\"user\", task_formatted)]}\n",
    "    )\n",
    "    return {\n",
    "        \"past_steps\": (task, agent_response[\"messages\"][-1].content),\n",
    "    }\n",
    "\n",
    "\n",
    "async def plan_step(state: PlanExecute):\n",
    "    plan = await planner.ainvoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
    "    return {\"plan\": plan.steps}\n",
    "\n",
    "\n",
    "async def replan_step(state: PlanExecute):\n",
    "    output = await replanner.ainvoke(state)\n",
    "    if isinstance(output.action, Response):\n",
    "        return {\"response\": output.action.response}\n",
    "    else:\n",
    "        return {\"plan\": output.action.steps}\n",
    "\n",
    "\n",
    "def should_end(state: PlanExecute) -> Literal[\"agent\", \"__end__\"]:\n",
    "    if \"response\" in state and state[\"response\"]:\n",
    "        return \"__end__\"\n",
    "    else:\n",
    "        return \"agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e954cea0-5ccc-46c2-a27b-f5b7185b597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "\n",
    "workflow = StateGraph(PlanExecute)\n",
    "\n",
    "# Add the plan node\n",
    "workflow.add_node(\"planner\", plan_step)\n",
    "\n",
    "# Add the execution step\n",
    "workflow.add_node(\"agent\", execute_step)\n",
    "\n",
    "# Add a replan node\n",
    "workflow.add_node(\"replan\", replan_step)\n",
    "\n",
    "workflow.set_entry_point(\"planner\")\n",
    "\n",
    "# From plan we go to agent\n",
    "workflow.add_edge(\"planner\", \"agent\")\n",
    "\n",
    "# From agent, we replan\n",
    "workflow.add_edge(\"agent\", \"replan\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"replan\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_end,\n",
    ")\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7363e528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e994b349-8dba-4f93-be41-c4403b530385",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating FunctionDeclaration: Unknown field for Schema: oneOf\n"
     ]
    }
   ],
   "source": [
    "from vertexai.generative_models import FunctionDeclaration\n",
    "\n",
    "# Example input data with an invalid 'anyOf' field\n",
    "parameters = {\n",
    "    \"anyOf\": [\n",
    "        {\"type\": \"string\"},\n",
    "        {\"type\": \"number\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Correct the input data to match the expected schema\n",
    "# Assume the correct fields are 'oneOf' and 'type'\n",
    "corrected_parameters = {\n",
    "    \"oneOf\": [\n",
    "        {\"type\": \"string\"},\n",
    "        {\"type\": \"number\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    function_declaration = FunctionDeclaration(name=\"example\", parameters=corrected_parameters, description=\"An example function\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error creating FunctionDeclaration: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f1b49b2-41da-4b60-b87d-98e21afe4fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_google_vertexai in /opt/conda/envs/pytorch/lib/python3.10/site-packages (1.0.2)\n",
      "Collecting langchain_google_vertexai\n",
      "  Downloading langchain_google_vertexai-1.0.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.47.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from langchain_google_vertexai) (1.51.0)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0,>=2.14.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from langchain_google_vertexai) (2.14.0)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.42 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from langchain_google_vertexai) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (2.17.1)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (2.23.4)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (1.22.3)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (3.20.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (23.2)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (3.13.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (1.10.4)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (2.0.2)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (2.7.1)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (0.15)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.14.0->langchain_google_vertexai) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.14.0->langchain_google_vertexai) (2.6.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.14.0->langchain_google_vertexai) (2.31.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.14.0->langchain_google_vertexai) (1.5.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.42->langchain_google_vertexai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.42->langchain_google_vertexai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.42->langchain_google_vertexai) (0.1.56)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.42->langchain_google_vertexai) (8.2.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (1.61.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (1.63.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (4.9)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (0.12.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.42->langchain_google_vertexai) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.42->langchain_google_vertexai) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=2.14.0->langchain_google_vertexai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=2.14.0->langchain_google_vertexai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=2.14.0->langchain_google_vertexai) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=2.14.0->langchain_google_vertexai) (2023.7.22)\n",
      "Requirement already satisfied: numpy>=1.14 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (1.25.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (0.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain_google_vertexai) (1.16.0)\n",
      "Downloading langchain_google_vertexai-1.0.4-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langchain_google_vertexai\n",
      "  Attempting uninstall: langchain_google_vertexai\n",
      "    Found existing installation: langchain-google-vertexai 1.0.2\n",
      "    Uninstalling langchain-google-vertexai-1.0.2:\n",
      "      Successfully uninstalled langchain-google-vertexai-1.0.2\n",
      "Successfully installed langchain_google_vertexai-1.0.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade langchain_google_vertexai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f71eebfc-5380-4712-98c2-d5397b30bd0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_format_to_vertex_tool' from 'langchain_google_vertexai' (/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_google_vertexai/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvertexai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerative_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionDeclaration\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_google_vertexai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _format_to_vertex_tool\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseTool\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# ... your existing code ...\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Modify the _format_to_vertex_tool function\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_format_to_vertex_tool' from 'langchain_google_vertexai' (/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_google_vertexai/__init__.py)"
     ]
    }
   ],
   "source": [
    "from vertexai.generative_models import FunctionDeclaration\n",
    "from langchain_google_vertexai import _format_to_vertex_tool\n",
    "from langchain.tools.base import BaseTool\n",
    "\n",
    "# ... your existing code ...\n",
    "\n",
    "# Modify the _format_to_vertex_tool function\n",
    "def _format_to_vertex_tool(tool: BaseTool) -> dict:\n",
    "    if isinstance(tool, BaseTool):\n",
    "        # Extract and adjust parameters for Vertex AI\n",
    "        parameters = tool.args\n",
    "        for param_name, param_info in parameters.items():\n",
    "            if \"anyOf\" in param_info:\n",
    "                parameters[param_name][\"oneOf\"] = param_info.pop(\"anyOf\")  # Replace 'anyOf' with 'oneOf'\n",
    "\n",
    "        return {\n",
    "            \"name\": tool.name,\n",
    "            \"description\": tool.description,\n",
    "            \"parameters\": parameters,  # Adjusted parameters\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected tool type: {type(tool)}\")\n",
    "\n",
    "# ... the rest of your existing code ...\n",
    "\n",
    "# Modify the _gemini_params function within ChatVertexAI\n",
    "def _gemini_params(self, stop, stream, tools, functions, tool_config, safety_settings, **kwargs):\n",
    "    # ... existing code ...\n",
    "\n",
    "    if tools:\n",
    "        tools = [_format_to_vertex_tool(tool) for tool in tools]  # Use the adjusted function\n",
    "\n",
    "    # ... rest of existing code ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8ac1f67-e87a-427c-b4f7-44351295b788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan': ['Find the winner of the 2024 Australian Open', 'Find the hometown of the winner']}\n",
      "{'past_steps': ('Find the winner of the 2024 Australian Open', 'The winner of the 2024 Australian Open is Jannik Sinner.')}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Protocol message Schema has no \"anyOf\" field.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/proto/message.py:570\u001b[0m, in \u001b[0;36mMessage.__init__\u001b[0;34m(self, mapping, ignore_unknown_fields, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 570\u001b[0m     pb_value \u001b[38;5;241m=\u001b[39m \u001b[43mmarshal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_proto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpb_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# Underscores may be appended to field names\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# that collide with python or proto-plus keywords.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;66;03m# See related issue\u001b[39;00m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;66;03m# https://github.com/googleapis/python-api-core/issues/227\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/proto/marshal/marshal.py:226\u001b[0m, in \u001b[0;36mBaseMarshal.to_proto\u001b[0;34m(self, proto_type, value, strict)\u001b[0m\n\u001b[1;32m    225\u001b[0m     recursive_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(proto_type()\u001b[38;5;241m.\u001b[39mvalue)\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_proto(recursive_type, v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m value\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    228\u001b[0m pb_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_rule(proto_type\u001b[38;5;241m=\u001b[39mproto_type)\u001b[38;5;241m.\u001b[39mto_proto(value)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/proto/marshal/marshal.py:226\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    225\u001b[0m     recursive_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(proto_type()\u001b[38;5;241m.\u001b[39mvalue)\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_proto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecursive_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m value\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    228\u001b[0m pb_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_rule(proto_type\u001b[38;5;241m=\u001b[39mproto_type)\u001b[38;5;241m.\u001b[39mto_proto(value)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/proto/marshal/marshal.py:228\u001b[0m, in \u001b[0;36mBaseMarshal.to_proto\u001b[0;34m(self, proto_type, value, strict)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_proto(recursive_type, v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m value\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 228\u001b[0m pb_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_rule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproto_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproto_type\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_proto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Sanity check: If we are in strict mode, did we get the value we want?\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/proto/marshal/rules/message.py:36\u001b[0m, in \u001b[0;36mMessageRule.to_proto\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Try the fast path first.\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_descriptor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# If we have a type error,\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# try the slow path in case the error\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# was an int64/string issue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Protocol message Schema has no \"anyOf\" field.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m50\u001b[39m}\n\u001b[1;32m      2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is the hometown of the 2024 Australia open winner?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m app\u001b[38;5;241m.\u001b[39mastream(inputs, config\u001b[38;5;241m=\u001b[39mconfig):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m event\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__end__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1216\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# wait for all background tasks to finish\u001b[39;00m\n\u001b[0;32m-> 1216\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mbg)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1114\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m   1107\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m   1108\u001b[0m     futures,\n\u001b[1;32m   1109\u001b[0m     return_when\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION,\n\u001b[1;32m   1110\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1111\u001b[0m )\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m-> 1114\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;66;03m# combine pending writes from all tasks\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m pending_writes \u001b[38;5;241m=\u001b[39m deque[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1342\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m   1340\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1341\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1342\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m   1347\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langgraph/pregel/retry.py:114\u001b[0m, in \u001b[0;36marun_with_retry\u001b[0;34m(task, retry_policy, stream)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mainvoke(task\u001b[38;5;241m.\u001b[39minput, task\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/runnables/base.py:2405\u001b[0m, in \u001b[0;36mRunnableSequence.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2403\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2404\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2405\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m   2406\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2407\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m   2408\u001b[0m             patch_config(\n\u001b[1;32m   2409\u001b[0m                 config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2410\u001b[0m             ),\n\u001b[1;32m   2411\u001b[0m         )\n\u001b[1;32m   2412\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langgraph/utils.py:115\u001b[0m, in \u001b[0;36mRunnableCallable.ainvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    111\u001b[0m         ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(\n\u001b[1;32m    112\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), context\u001b[38;5;241m=\u001b[39mcontext\n\u001b[1;32m    113\u001b[0m         )\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m         ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m, in \u001b[0;36mreplan_step\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplan_step\u001b[39m(state: PlanExecute):\n\u001b[0;32m---> 24\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m replanner\u001b[38;5;241m.\u001b[39mainvoke(state)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output\u001b[38;5;241m.\u001b[39maction, Response):\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m: output\u001b[38;5;241m.\u001b[39maction\u001b[38;5;241m.\u001b[39mresponse}\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/runnables/base.py:2405\u001b[0m, in \u001b[0;36mRunnableSequence.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2403\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2404\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2405\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m   2406\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2407\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m   2408\u001b[0m             patch_config(\n\u001b[1;32m   2409\u001b[0m                 config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2410\u001b[0m             ),\n\u001b[1;32m   2411\u001b[0m         )\n\u001b[1;32m   2412\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/runnables/base.py:4408\u001b[0m, in \u001b[0;36mRunnableBindingBase.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4402\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m   4403\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4404\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4405\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4406\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4407\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m   4409\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   4410\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   4411\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   4412\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:191\u001b[0m, in \u001b[0;36mBaseChatModel.ainvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    189\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    190\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m--> 191\u001b[0m     llm_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate_prompt(\n\u001b[1;32m    192\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    193\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    194\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    195\u001b[0m         tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    196\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    197\u001b[0m         run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    198\u001b[0m         run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ChatGeneration, llm_result\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:609\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magenerate_prompt\u001b[39m(\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    603\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    607\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    608\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 609\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate(\n\u001b[1;32m    610\u001b[0m         prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    611\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:569\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    558\u001b[0m             \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m    559\u001b[0m                 run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    567\u001b[0m             ]\n\u001b[1;32m    568\u001b[0m         )\n\u001b[0;32m--> 569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    570\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    571\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item, union-attr]\u001b[39;00m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    573\u001b[0m ]\n\u001b[1;32m    574\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:754\u001b[0m, in \u001b[0;36mBaseChatModel._agenerate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 754\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(\n\u001b[1;32m    755\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    756\u001b[0m         )\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    758\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_google_vertexai/chat_models.py:676\u001b[0m, in \u001b[0;36mChatVertexAI._agenerate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate_non_gemini(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    675\u001b[0m client, contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gemini_client_and_contents(messages)\n\u001b[0;32m--> 676\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gemini_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m telemetry\u001b[38;5;241m.\u001b[39mtool_context_manager(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_agent):\n\u001b[1;32m    678\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _acompletion_with_retry(\n\u001b[1;32m    679\u001b[0m         client\u001b[38;5;241m.\u001b[39mgenerate_content_async,\n\u001b[1;32m    680\u001b[0m         max_retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    681\u001b[0m         contents\u001b[38;5;241m=\u001b[39mcontents,\n\u001b[1;32m    682\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    683\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_google_vertexai/chat_models.py:986\u001b[0m, in \u001b[0;36mChatVertexAI._gemini_params\u001b[0;34m(self, stop, stream, tools, functions, tool_config, safety_settings, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m generation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_params(stop\u001b[38;5;241m=\u001b[39mstop, stream\u001b[38;5;241m=\u001b[39mstream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tools:\n\u001b[0;32m--> 986\u001b[0m     tools \u001b[38;5;241m=\u001b[39m [_format_to_vertex_tool(tool) \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools]\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m functions:\n\u001b[1;32m    988\u001b[0m     tools \u001b[38;5;241m=\u001b[39m [_format_to_vertex_tool(functions)]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_google_vertexai/chat_models.py:986\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    984\u001b[0m generation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_params(stop\u001b[38;5;241m=\u001b[39mstop, stream\u001b[38;5;241m=\u001b[39mstream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tools:\n\u001b[0;32m--> 986\u001b[0m     tools \u001b[38;5;241m=\u001b[39m [\u001b[43m_format_to_vertex_tool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools]\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m functions:\n\u001b[1;32m    988\u001b[0m     tools \u001b[38;5;241m=\u001b[39m [_format_to_vertex_tool(functions)]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_google_vertexai/functions_utils.py:123\u001b[0m, in \u001b[0;36m_format_to_vertex_tool\u001b[0;34m(tool)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[1;32m    117\u001b[0m     tool \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    118\u001b[0m         _format_functions_to_vertex_tool_dict(tool)\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool, \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m tool\n\u001b[1;32m    121\u001b[0m     )\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VertexTool(\n\u001b[0;32m--> 123\u001b[0m         function_declarations\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    124\u001b[0m             FunctionDeclaration(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfd) \u001b[38;5;28;01mfor\u001b[39;00m fd \u001b[38;5;129;01min\u001b[39;00m tool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_declarations\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    125\u001b[0m         ]\n\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected tool value:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtool\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_google_vertexai/functions_utils.py:124\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[1;32m    117\u001b[0m     tool \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    118\u001b[0m         _format_functions_to_vertex_tool_dict(tool)\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool, \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m tool\n\u001b[1;32m    121\u001b[0m     )\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VertexTool(\n\u001b[1;32m    123\u001b[0m         function_declarations\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m--> 124\u001b[0m             \u001b[43mFunctionDeclaration\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfd\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m fd \u001b[38;5;129;01min\u001b[39;00m tool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_declarations\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    125\u001b[0m         ]\n\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected tool value:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtool\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/vertexai/generative_models/_generative_models.py:1531\u001b[0m, in \u001b[0;36mFunctionDeclaration.__init__\u001b[0;34m(self, name, parameters, description)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a FunctionDeclaration.\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \n\u001b[1;32m   1524\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;124;03m        Model uses it to decide how and whether to call the function.\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m gapic_schema_dict \u001b[38;5;241m=\u001b[39m _convert_schema_dict_to_gapic(parameters)\n\u001b[0;32m-> 1531\u001b[0m raw_schema \u001b[38;5;241m=\u001b[39m \u001b[43maiplatform_types\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgapic_schema_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_function_declaration \u001b[38;5;241m=\u001b[39m gapic_tool_types\u001b[38;5;241m.\u001b[39mFunctionDeclaration(\n\u001b[1;32m   1533\u001b[0m     name\u001b[38;5;241m=\u001b[39mname, description\u001b[38;5;241m=\u001b[39mdescription, parameters\u001b[38;5;241m=\u001b[39mraw_schema\n\u001b[1;32m   1534\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/proto/message.py:598\u001b[0m, in \u001b[0;36mMessage.__init__\u001b[0;34m(self, mapping, ignore_unknown_fields, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m keys_to_update:\n\u001b[1;32m    596\u001b[0m             value[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mpop(item)\n\u001b[0;32m--> 598\u001b[0m     pb_value \u001b[38;5;241m=\u001b[39m \u001b[43mmarshal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_proto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpb_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pb_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    601\u001b[0m     params[key] \u001b[38;5;241m=\u001b[39m pb_value\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/proto/marshal/marshal.py:226\u001b[0m, in \u001b[0;36mBaseMarshal.to_proto\u001b[0;34m(self, proto_type, value, strict)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    222\u001b[0m     proto_type\u001b[38;5;241m.\u001b[39mDESCRIPTOR\u001b[38;5;241m.\u001b[39mhas_options\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m proto_type\u001b[38;5;241m.\u001b[39mDESCRIPTOR\u001b[38;5;241m.\u001b[39mGetOptions()\u001b[38;5;241m.\u001b[39mmap_entry\n\u001b[1;32m    224\u001b[0m ):\n\u001b[1;32m    225\u001b[0m     recursive_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(proto_type()\u001b[38;5;241m.\u001b[39mvalue)\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_proto(recursive_type, v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m value\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    228\u001b[0m pb_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_rule(proto_type\u001b[38;5;241m=\u001b[39mproto_type)\u001b[38;5;241m.\u001b[39mto_proto(value)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Sanity check: If we are in strict mode, did we get the value we want?\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/proto/marshal/marshal.py:226\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    222\u001b[0m     proto_type\u001b[38;5;241m.\u001b[39mDESCRIPTOR\u001b[38;5;241m.\u001b[39mhas_options\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m proto_type\u001b[38;5;241m.\u001b[39mDESCRIPTOR\u001b[38;5;241m.\u001b[39mGetOptions()\u001b[38;5;241m.\u001b[39mmap_entry\n\u001b[1;32m    224\u001b[0m ):\n\u001b[1;32m    225\u001b[0m     recursive_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(proto_type()\u001b[38;5;241m.\u001b[39mvalue)\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_proto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecursive_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m value\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    228\u001b[0m pb_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_rule(proto_type\u001b[38;5;241m=\u001b[39mproto_type)\u001b[38;5;241m.\u001b[39mto_proto(value)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Sanity check: If we are in strict mode, did we get the value we want?\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/proto/marshal/marshal.py:228\u001b[0m, in \u001b[0;36mBaseMarshal.to_proto\u001b[0;34m(self, proto_type, value, strict)\u001b[0m\n\u001b[1;32m    225\u001b[0m     recursive_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(proto_type()\u001b[38;5;241m.\u001b[39mvalue)\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_proto(recursive_type, v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m value\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 228\u001b[0m pb_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_rule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproto_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproto_type\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_proto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Sanity check: If we are in strict mode, did we get the value we want?\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pb_value, proto_type):\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/proto/marshal/rules/message.py:36\u001b[0m, in \u001b[0;36mMessageRule.to_proto\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_map:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# We need to use the wrapper's marshaling to handle\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# potentially problematic nested messages.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;66;03m# Try the fast path first.\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_descriptor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;66;03m# If we have a type error,\u001b[39;00m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;66;03m# try the slow path in case the error\u001b[39;00m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;66;03m# was an int64/string issue\u001b[39;00m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapper(value)\u001b[38;5;241m.\u001b[39m_pb\n",
      "\u001b[0;31mValueError\u001b[0m: Protocol message Schema has no \"anyOf\" field."
     ]
    }
   ],
   "source": [
    "config = {\"recursion_limit\": 50}\n",
    "inputs = {\"input\": \"what is the hometown of the 2024 Australia open winner?\"}\n",
    "async for event in app.astream(inputs, config=config):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf585a9-0f1e-4910-bd00-65e7bb05b6e6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congrats on making a plan-and-execute agent! One known limitations of the above design is that each task is still executed in sequence, meaning embarrassingly parallel operations all add to the total execution time. You could improve on this by having each task represented as a DAG (similar to LLMCompiler), rather than a regular list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8f7955-2cc9-4ebb-8c41-13abb3351a24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13 (Local)",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
